<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <title>Group 23 - NLP</title>

  <!-- CSS only -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">
  <link href="/docs/5.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">

  <!-- Favicons -->

  <!-- Custom styles for this template -->
  <link href="css/styles.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>

<body>

  <!-- Navbar -->
  <div class="container" style="max-width: 800px">
    <nav class="navbar mx-auto bg-light px-4 fixed-top">
      <a class="navbar-brand" href="index.html">Big Data and Cloud Computing - Final Project</a>
      <ul class="nav nav-pills justify-content-center">
        <li class="nav-item"><a class="nav-link" href="index.html#projectIntroduction" style="color: black">Project
            Introduction</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#aboutTeam" style="color: black">About the Team</a>
        </li>
        <li class="nav-item"><a class="nav-link" href="summary.html" style="color: black">Executive Summary</a></li>
        <li class="nav-item"><a class="nav-link" href="EDA.html" style="color: black">EDA</a></li>
        <li class="nav-item"><a class="nav-link" href="NLP.html" style="color: black">NLP</a></li>
        <li class="nav-item"><a class="nav-link" href="ML.html" style="color: black">Machine Learning</a></li>
      </ul>
    </nav>





    <br>
    <h2>Machine Learning</h2>
    <hr>
    <br>
    <h4>Summary</h4>
    <br>
    <p>
      Reddit is a fruitful online community full of opinions on different topics. From using NLP techniques, we extract
      a suite of topic and sentiment features, and subset language-specific reddit posts. We are able to use
      bag-of-words features to predict Ukrainian reddit scores with a 95% accuracy.
    </p>
    <div class="col-md-10">
      <div class="card p-0" style="border: none">
        <img src="img/ML/comment_model.png" class="card-img-top" alt="...">
        <div class="card-body">
          <p style="font-weight: lighter; text-align: center;"><b>FIGURE 1 </b>Model Performance from Comment Features<a
              href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/blob/master/code/ML/Visualization.ipynb">Visualization
              Code</a>.</p>

        </div>
      </div>
    </div>
    <p>
      We also try to predict commodity returns with sentiment and topic features. Financial assets returns are
      notoriously hard to predict. A positive R^2 in modeling means a better-than-guess prediction, and if the positive
      R^2 stays long enough, it can be built into a trading signal. As Figure shows, even though most of our models can
      only generate negative R^2, we
      are still able
      to make a positive R^2 prediction using Gradient Boosting. This result is two-fold. On the one hand, reddit data
      has a low information-to-noise ratio for it to be used in commodity returns prediction. On the other hand, it
      still can be used to generate decent prediction with advanced modeling.
    </p>
    <br>
    <hr>

    <h4>Q5. Can Foreign Language Comments be used to Predict Comment Score? Using Transformation, Pipeline, and SparkML.
    </h4>
    <p>
      We split the training and testing dataset by 0.7 and 0.3 ratio from Ukrainian comment dataset, with 12859 and 5462
      posts
      respectively. We utilized SparkML and Spark Pipeline to streamline the tokenization, word vectorization, and
      machine learning process. Logistic
      regression has the best performance with a 95.75 Accuracy.
      Our conclusion is that
      bag-of-words approach text classification methods can be applied to any languages that use space to separate
      words, and perform well. The code can be found <a
        href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/blob/master/code/ML/Predict_Comment_Score.ipynb">here</a>.
    </p>
    <br>
    <hr>
    <h4>Q7, Q9, & Q10. How to use Redit Topics and Sentiments to Predict Commodity Returns?</h4>
    <br>
    <h4>Models</h4>
    <br>
    <h4>Gradient Boosting</h4>

    <p>
      <b>Boosting</b> refers to modifying a weak learner or a weak hypothesis to make it perform better.
      A <b>weak hypothesis</b> or a <b>weak learner</b> is defined as one whose performance is at least
      slightly better than random chance.
    </p>
    <p>
      In gradient boosting, decision trees are used as the weak learner and are parameterized using
      gradient descent. Each tree obtains the instances it handles well and leaves the hard-to-handle
      instances to the following trees. The model adds a fixed number of tree-structured weak learners
      once at a time and stops when the loss reaches an acceptable level or no longer improves on
      external validation data. Thus, gradient boosting is a greedy algorithm that can quickly overfit
      a training dataset. We tune several key hyperparameters to add constraints to the training process.
    </p>
    <ul>
      <li><i>learning_rate</i>: The model shrinks the contribution of each tree by the specified
        learning rate. default is 0.1</li>
      <li><i>n_estimators</i>: The number of boosting stages to perform. More boosting stages
        add more trees to the model and perform fairly better. There is a considered trade-off
        between <i>learning_rate</i> and <i>n_estimators</i>, but we give a fair amount of
        attempts for both parameters in the hope of the best performance.</li>
      <li><i>max_depth</i>: The maximum depth limit the number of nodes in the tree. And it can
        start with one node per tree.</li>
      <li><i>subsample</i>: The value determines the fraction of samples to be used from
        fitting individual base learners.</li>
    </ul>
    <p>We first set up lists of these parameters in their acceptable range with large intervals. After
      looping through all combinations, we obtain the best set of parameters. We then derive new lists
      around the result with smaller intervals. Using this method, we get close to the actual range of
      possible best estimators. Although the models using different combinations are obtained through
      training and validation datasets, we filter the estimators with the highest r-squared score from the
      prediction on the test dataset to avoid overfitting.
    </p>

    <!-- <h4>Result</h4>
    <p>
      We insert all the hyperparameter characters and values into our tuning function and obtain the model
      with the best prediction result. Our optimal gradient-boosting model uses only sentiment scores and
      topic scores from the comments. Although we test different possible dimension-reduced datasets using
      PCA, the transformation does not affect the result much. We remove the PCA process for our model to
      save training time and computational power. The best estimators have learning rates of around 0.001,
      maximum depths of about 100, and numbers of estimators of about 50. We give approximate values of
      parameters since the models with parameters that differ in small scales tend to deliver similar good
      results. And we consider the focus of generating the ideal model should be on following the
      hyperparameter tuning workflow instead of trying to fit a fixed set of parameters to data.
      The code used for the gradient boosting model can be found <a
        href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/blob/master/code/ML/Gradient_Boosting Rank.ipynb">here</a>.
    </p>
    <p>
      Using our optimized gradient boosting model, we are able to achieve a positive r-squared score on
      the prediction of our testing dataset at around 0.01257. Based on the hard-to-predict nature of
      the commodity returns, the model performs well for our hypothesis.
    </p>
    <p>
      We also obtain the prediction for each commodity from our optimal model. The r-squared score of
      <b>Heating Oil</b>, <b>Wheat</b>, <b>Coffe</b>, <b>Natural Gas</b>, <b>Rough Rice</b>, <b>Soybean
        Oil Futures</b>, <b>Sugar #11</b> are all positive and higher than 0 by around 0.01. Among all
      the categories, <b>Soybean Meal Futures</b> has the highest score at around 0.173. Thus we
      conclude that the optimal gradient boosting model performs exceptionally well on the return
      of <b>Soybean Meal Futures</b>.
    </p> -->

    <br>

    <hr>

    <h4>Random Forest Regressor</h4>

    <p>
      Random Forest is another ensemble method that enhances learning by grouping weak learners to form a single strong
      learner. The robustness of Random Forest comes from the wisdom of crowds and works well on large datasets. Also,
      decision trees are goot at capturing non-linear relationships between input features and the target variable.
    </p>

    <p>
      We use Random Forest Regressor model to predict commodity returns. Similar to Boosting, the parameters tuned in
      the model are n_estimators and max_depth. Then we set up lists of these parameters and loop through all
      vombinations to get the best set of parameters. Finally, we filter out the estimators with the highest r-squared
      score.
    </p>

    <ul>
      <li><i>n_estimators</i>: the number of trees in the forest</li>
      <li><i>max_depth</i>: The maximum depth limit the number of nodes in the tree</li>
    </ul>

    <!-- <h4>Result</h4>


    <div class="table-responsive">
      <table class="table">
        <thead>
          <tr>
            <th scope="col" style="width: 40%; text-align: center;">Commodity</th>
            <th scope="col" style="width: 20%; text-align: center;">Comment</th>
            <th scope="col" style="width: 20%; text-align: center;">Submission</th>
            <th scope="col" style="width: 20%; text-align: center;">Comment+Submission</th>
          </tr>
        </thead>
        <tbody style="text-align: center;">
          <tr>
            <td>Corn</td>
            <td>-0.15</td>
            <td>-0.45</td>
            <td>-0.32</td>
          </tr>

          <tr>
            <td>Crude Oil</td>
            <td>-0.13</td>
            <td>-0.08</td>
            <td>-0.13</td>
          </tr>

          <tr>
            <td>Heating Oil</td>
            <td>-0.15</td>
            <td>-0.08</td>
            <td>-0.18</td>
          </tr>

          <tr>
            <td>Wheat</td>
            <td>-0.20</td>
            <td>-0.35</td>
            <td>-0.18</td>
          </tr>

          <tr>
            <td>Cocoa</td>
            <td>-0.14</td>
            <td>-0.12</td>
            <td>0.02</td>
          </tr>

          <tr>
            <td>Coffee</td>
            <td>-0.08</td>
            <td>-0.20</td>
            <td>-0.18</td>
          </tr>

          <tr>
            <td>Natural Gas</td>
            <td>-0.12</td>
            <td>0.22</td>
            <td>0.02</td>
          </tr>

          <tr>
            <td>Oats</td>
            <td>-0.05</td>
            <td>-0.18</td>
            <td>-0.25</td>
          </tr>

          <tr>
            <td>RBOB Gasoline</td>
            <td>-0.02</td>
            <td>-0.10</td>
            <td>-0.12</td>
          </tr>

          <tr>
            <td>Rough Rice</td>
            <td>-0.12</td>
            <td>-0.14</td>
            <td>-0.14</td>
          </tr>

          <tr>
            <td>Soybean Meal</td>
            <td>-0.10</td>
            <td>-0.13</td>
            <td>-0.06</td>
          </tr>

          <tr>
            <td>Soybean Oil</td>
            <td>-0.08</td>
            <td>-0.14</td>
            <td>-0.03</td>
          </tr>

          <tr>
            <td>Sugar #11</td>
            <td>-0.11</td>
            <td>-0.14</td>
            <td>-0.23</td>
          </tr>

          <tr>
            <td>Newcastle Coal</td>
            <td>-0.01</td>
            <td>0.06</td>
            <td>0.11</td>
          </tr>

          <tr>
            <td>Barley</td>
            <td>-0.53</td>
            <td>-0.25</td>
            <td>-0.32</td>
          </tr>





        </tbody>

      </table>
      <p style="font-weight: lighter; text-align: center;"><b>TABLE 1:</b> Selected subreddits</p>

    </div> -->

    <br>
    <hr>

    <h4>Feedforward Neural Network</h4>
    <p>
      <strong>Feedforward Neural Network</strong> is a group of models, with use of linear layers and activation.
      It could be used to model non-linear relationships and dig into the deep insights of a dataset.
      Gradient descent based optimizers and schedulers are used for the training process.
      Neural Network is different from the previous two methods. It requires a large number of choices for training
      hyper-parameters.
    </p>
    <p>
      To avoid overfittiing, we simply maintain 2 hidden layers for the network, with the units of the 2nd
      hidden layer being 2 times the units of the 1st hidden layer.
    </p>
    <p>
      We use a Adam optimizer, a linear scheduler with warmups and the MSE loss function
      for the optimization program.
    </p>
    <p>
      We first try to find the best choice of hyper-parameters according to r-squared scores,
      by training the model on the training dataset and then do validation on the validation dataset.
      With the best choice of hyper-parameters, we do formal training on the overall training dataset and
      get the r-squared scores on the testing dataset.
    </p>
    <ul>
      <li><i>base_hidden_layer_units</i>: The base number of units for the first hidden layer.</li>
      <li><i>learning_rate</i>: The variable deciding the scale of gradient changes applied to network
        parameters.</li>
      <li><i>epochs</i>: The time of optimization processes on the training dataset repeatedly.</li>
      <li><i>activation</i>: The choice of activation function for hidden layers.</li>
      <li><i>batch_size</i>: Size of batch to compute the loss. Updating parameters using the loss computed
        on a batch of data points prevents the network from going into some local optimal regions.</li>
      <li><i>max_grad_norm</i>: The value used to clip the parameter gradients before each update.</li>
      <li><i>num_warmup_steps</i>: Warmup steps for the scheduler. Default 3.</li>
    </ul>
    <!-- <h4>Result</h4>
    <p>
      We found that with proper initialization and training processes on the comment features,
      we can achieve good results in 12 commodities:
      <strong>Natural Gas</strong>, <strong>RBOB Gasoline</strong>, <strong>Oats</strong>,
      <strong>Heating Oil</strong>, <strong>Corn Futures</strong>, <strong>Crude Oil</strong>,
      <strong>Soybean Meal Futures</strong>, <strong>Wheat</strong>, <strong>Soybean Oil Futures</strong>,
      <strong>Coffee</strong>, <strong>Newcastle Coal</strong>, <strong>Barley</strong>.
    </p>

    <p>The summary table of the best-case r-squared scores is listed below:(Sorted by testing R2 score)</p>

    <table class="table">
      <thead>
        <tr>
          <th scope="col" style="width: 20%; text-align: center;">Commodity</th>
          <th scope="col" style="width: 20%; text-align: center;">Testing R2 Score</th>
          <th scope="col" style="width: 20%; text-align: center;">Training R2 Score</th>
        </tr>
      </thead>
      <tbody style="text-align: center;">
        <tr>
          <td>Natural Gas</td>
          <td>0.289160</td>
          <td>0.120401</td>
        </tr>
        <tr>
          <td>RBOB Gasoline</td>
          <td>0.217282</td>
          <td>0.088567</td>
        </tr>
        <tr>
          <td>Oats</td>
          <td>0.139280</td>
          <td>0.075165</td>
        </tr>
        <tr>
          <td>Heating Oil</td>
          <td>0.131966</td>
          <td>0.030550</td>
        </tr>
        <tr>
          <td>Corn</td>
          <td>0.120237</td>
          <td>0.057378</td>
        </tr>
        <tr>
          <td>Crude Oil</td>
          <td>0.116677</td>
          <td>0.093161</td>
        </tr>
        <tr>
          <td>Soybean Meal</td>
          <td>0.111873</td>
          <td>0.052304</td>
        </tr>
        <tr>
          <td>Wheat</td>
          <td>0.104399</td>
          <td>0.280816</td>
        </tr>
        <tr>
          <td>Soybean Oil</td>
          <td>0.055231</td>
          <td>0.010789</td>
        </tr>
        <tr>
          <td>Coffee</td>
          <td>0.046736</td>
          <td>0.030898</td>
        </tr>
        <tr>
          <td>Newcastle Coal</td>
          <td>0.042269</td>
          <td>0.039902</td>
        </tr>
        <tr>
          <td>Barley</td>
          <td>0.029870</td>
          <td>0.032735</td>
        </tr>
      </tbody>
    </table>
    <p style="font-weight: lighter; text-align: center;"><b>TABLE 2:</b> The Best R-squared Scores Achieved from
      Feedforward Neural Network</p> -->
    <p>All the models code can be found <a
        href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/tree/master/code/ML">here.</a>
    </p>
    <br>

    <hr>

    <h4>Results</h4>

    <div class="row py-3">
      <div class="col-md-12">
        <div class="row">
          <div class="col-md-6 img-responsive">
            <img src="img/ML/comment_model.png" class="card-img-top" alt="...">
          </div>
          <div class="col-md-6 img-responsive">
            <img src="img/ML/submission_model.png" class="card-img-top" alt="...">
          </div>
          <div class="card-body">
            <p style="font-weight: lighter; text-align: center;"><strong>FIGURE 1 </strong>Performance of Different ML
              Models. The left figure plots the performance using comment-generated features. The right figure plots the
              performance using submission-generated features.<br>
              <a
                href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/blob/master/code/ML/Visualization.ipynb">Visualization
                Code</a>
            </p>
          </div>
        </div>
      </div>
    </div>
    <p>
      Because the market is very efficient, financial asset's returns are hard to predict. Even though most of the
      models do not work well, gradient boosting is able to generate positive returns. And we notice that
      comment-generated features generally have better results than submission-generated features.
    </p>

    <br>

    <hr>
    <br>
  </div> <!-- container -->

  <footer class="blog-footer">
    <p>
      <a href="#">Back to top</a>
    </p>
  </footer>

</body>

</html>