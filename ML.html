<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <title>Group 23 - NLP</title>

  <!-- CSS only -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">
  <link href="/docs/5.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">

  <!-- Favicons -->

  <!-- Custom styles for this template -->
  <link href="css/styles.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>

<body>

  <!-- Navbar -->
  <div class="container" style="max-width: 800px">
    <nav class="navbar mx-auto bg-light px-4 fixed-top">
      <a class="navbar-brand" href="index.html">Big Data and Cloud Computing - Final Project</a>
      <ul class="nav nav-pills justify-content-center">
        <li class="nav-item"><a class="nav-link" href="index.html#projectIntroduction" style="color: black">Project
            Introduction</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#aboutTeam" style="color: black">About the Team</a>
        </li>
        <li class="nav-item"><a class="nav-link" href="summary.html" style="color: black">Executive Summary</a></li>
        <li class="nav-item"><a class="nav-link" href="EDA.html" style="color: black">EDA</a></li>
        <li class="nav-item"><a class="nav-link" href="NLP.html" style="color: black">NLP</a></li>
        <li class="nav-item"><a class="nav-link" href="ML.html" style="color: black">Machine Learning</a></li>
      </ul>
    </nav>





    <br>
    <h2>Machine Learning</h2>
    <hr>
    <br>

    <h3>Gradient Boosting</h3>
    <h4>Model</h4>
    <p>
      <b>Boosting</b> refers to modifying a weak learner or a weak hypothesis to make it perform better. 
      A <b>weak hypothesis</b> or a <b>weak learner</b> is defined as one whose performance is at least 
      slightly better than random chance. Commodity market returns are conventionally considered to be 
      hard to model. We build our work based on the hypothesis that this data can be predicted using 
      specific data and designated models. In other words, our work initializes with a weak hypothesis. 
      And gradient descent is meant for this learning process. 
    </p>
    <p>
      In gradient boosting, decision trees are used as the weak learner and are parameterized using 
      gradient descent. Each tree obtains the instances it handles well and leaves the hard-to-handle 
      instances to the following trees. The model adds a fixed number of tree-structured weak learners 
      once at a time and stops when the loss reaches an acceptable level or no longer improves on 
      external validation data. Thus, gradient boosting is a greedy algorithm that can quickly overfit 
      a training dataset. We tune several key hyperparameters to add constraints to the training process.
    </p>
    <ul>
      <li><i>learning_rate</i>: The model shrinks the contribution of each tree by the specified 
        learning rate. default is 0.1</li>
      <li><i>n_estimators</i>: The number of boosting stages to perform. More boosting stages 
        add more trees to the model and perform fairly better. There is a considered trade-off 
        between <i>learning_rate</i> and <i>n_estimators</i>, but we give a fair amount of 
        attempts for both parameters in the hope of the best performance.</li>
      <li><i>max_depth</i>: The maximum depth limit the number of nodes in the tree. And it can 
        start with one node per tree.</li>
      <li><i>subsample</i>: The value determines the fraction of samples to be used from 
        fitting individual base learners.</li>
    </ul>
    <p>We first set up lists of these parameters in their acceptable range with large intervals. After 
      looping through all combinations, we obtain the best set of parameters. We then derive new lists 
      around the result with smaller intervals. Using this method, we get close to the actual range of 
      possible best estimators. Although the models using different combinations are obtained through 
      training and validation datasets, we filter the estimators with the highest r-squared score from the 
      prediction on the test dataset to avoid overfitting.
    </p>
    <p>
      On top of the tuning loop, we test different sets of variables to compare the testing score to 
      find the optimal set of input variables. Based on the numbers of various input feature sets, we 
      fit a range of PCA models with different limitations of components to the dataset before splitting 
      it.
    </p>
    <p>
      Moreover, We use the standard scaler over all the features in the dataset before splitting it into 
      training, validation, and test subsets. 
    </p>
    
    <h4>Result</h4>
    <p>
      We insert all the hyperparameter characters and values into our tuning function and obtain the model 
      with the best prediction result. Our optimal gradient-boosting model uses only sentiment scores and 
      topic scores from the comments. Although we test different possible dimension-reduced datasets using 
      PCA, the transformation does not affect the result much. We remove the PCA process for our model to 
      save training time and computational power. The best estimators have learning rates of around 0.001, 
      maximum depths of about 100, and numbers of estimators of about 50. We give approximate values of 
      parameters since the models with parameters that differ in small scales tend to deliver similar good 
      results. And we consider the focus of generating the ideal model should be on following the 
      hyperparameter tuning workflow instead of trying to fit a fixed set of parameters to data. 
      The code used for the gradient boosting model can be found <a
        href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/blob/master/code/ML/Gradient_Boosting Rank.ipynb">here</a>.
    </p>
    <p>
      Using our optimized gradient boosting model, we are able to achieve a positive r-squared score on 
      the prediction of our testing dataset at around 0.01257. Based on the hard-to-predict nature of 
      the commodity returns, the model performs well for our hypothesis.
    </p>
    <p>
      We also obtain the prediction for each commodity from our optimal model. The r-squared score of 
      <b>Heating Oil</b>,  <b>Wheat</b>, <b>Coffe</b>, <b>Natural Gas</b>, <b>Rough Rice</b>, <b>Soybean 
        Oil Futures</b>, <b>Sugar #11</b> are all positive and higher than 0 by around 0.01. Among all 
        the categories, <b>Soybean Meal Futures</b> has the highest score at around 0.173. Thus we 
        conclude that the optimal gradient boosting model performs exceptionally well on the return 
        of <b>Soybean Meal Futures</b>.
    </p>

    <hr>

    <br>


    <h4>Q5. Can Foreign Language Comments be used to Predict Comment Score? Using Transformation, Pipeline, and SparkML.
    </h4>
    <p>
      We split the training and testing dataset by 0.7 and 0.3 ratio from Ukrainian comment dataset, with 12859 and 5462 posts
      respectively. We utilized SparkML and Spark Pipeline to streamline the tokenization, word vectorization, and
      machine learning process. Logistic
      regression has the best performance with a 95.75 Accuracy.
      Our conclusion is that
      bag-of-words approach text classification methods can be applied to any languages that use space to separate
      words, and perform well. The code can be found <a
        href="https://github.com/gu-anly502/fall-2022-reddit-big-data-project-project-group-23/blob/master/code/ML/Predict_Comment_Score.ipynb">here</a>.
    </p>

    <br>

    <hr>
    <br>
  </div> <!-- container -->

  <footer class="blog-footer">
    <p>
      <a href="#">Back to top</a>
    </p>
  </footer>

</body>

</html>
