<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <title>Group 23 - EDA</title>

  <!-- CSS only -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">
  <link href="/docs/5.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">

  <!-- Favicons -->

  <!-- Custom styles for this template -->
  <link href="css/styles.css" rel="stylesheet">
</head>

<body>

  <!-- Navbar -->
  <div class="container" style="max-width: 800px">
    <nav class="navbar mx-auto bg-light px-4 fixed-top">
      <a class="navbar-brand" href="index.html">Big Data and Cloud Computing - Final Project</a>
      <ul class="nav nav-pills justify-content-center">
        <li class="nav-item"><a class="nav-link" href="index.html#projectIntroduction" style="color: black">Project
            Introduction</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#aboutTeam" style="color: black">About the Team</a>
        </li>
        <li class="nav-item"><a class="nav-link" href="EDA.html" style="color: black">EDA</a></li>
        <li class="nav-item"><a class="nav-link" href="NLP.html" style="color: black">NLP</a></li>
      </ul>
    </nav>





    <br>
    <h2>Natural Language Processing</h2>
    <hr>
    <br>

    <p>
      The goal of our analysis:
    <ol>
      <li>(Xinlu) Common words, Important words,.......</li>
      <li>Sentiment Analysis.........</li>
      <li>Topic Modeling......</li>
      <li>Combine NLP with external data</li>
    </ol>
    </p>
    <br>
    <br>
    <br>



    <hr>
    <h4>
      Data Cleaning
    </h4>

    <p>
      Data cleaning pipeline
    </p>


    <!-- IMAGE: schema -->
    <div class="row py-5">

      <div class="col-md-12">
        <div class="card p-0" style="border: none">
          <img src="img/NLP/temp.png" class="card-img-top" alt="...">
          <div class="card-body">
            <p style="font-weight: lighter; text-align: center;">Explanation</p>

          </div>
        </div>
      </div>

      <!-- IMAGE: schema END -->


      <!--
      <p><mark>Add figure number</mark></p>
      -->

    </div>




    <br>
    <br>
    <br>
    <!-- Q1 -->
    <hr>
    <h4>Q1. (NLP) What are the major topics discussed in Reddit related to Russia Ukraine conflict?</h4>
    <br>
    <p>
      <strong>Topic models</strong> generate interpretable text features extracted from documents. These models help identify and cluster similar documents and are useful tools to tag documents.
    </p>

    <p><a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html">Spark MLlib documentation</a> specifies the terminology used in this analysis: Link</p>
    <ol>
      <li><strong>term = word</strong>: an element of the vocabulary</li>
      <li><strong>token</strong>: instance of a term appearing in a document</li>
      <li><strong>topic</strong>:multinomial distribution over terms representing some concept</li>
      <li><strong>document</strong>: one piece of text, corresponding to one row in the input data</li>
    </ol>

    <br>






    <h4>
      Latent Dirichlet Allocation (LDA)
    </h4>
    <p>
      <strong>Latent Dirichlet Allocation (LDA)</strong> is a statistical model that helps produce meaningful topics that humans can relate to. It assumes that topics are probability distributions over words, and documents are distributions over topics and that topics use only a limited number of terms frequently. 
    </p>

    <ul>
      <li><strong>k</strong>: number of topics (= number of clusters)</li>
      <li><strong>maxIter</strong>: number of iterations</li>
      <li><strong>featuresCol</strong>: a collection of documents as input data. Feature transformers such as <strong>Tokenizer</strong> and <strong>CountVectorizer</strong> are used to convert text to word count vectors as input data.</li>
    </ul>
    <br>

    <h4>
      Topic Modeling Pipeline
    </h4>
    <p>
      We proceed with 15 topics, top 10 words per topic using the cleaned documents as input data. The <a href="https://colab.research.google.com/drive/14bf2kwdU4f5UBUYEkTJO01NjFtWEZU_-#scrollTo=yqwgYNlgwxvX">notebook</a> contains the LDA pipeline applied to the submissions and comments data. The data from two sources are stacked using <mark>union</mark> command to provide comprehensive topic modeling on every posts related to RU Conflict in the Reddit dataset. 
    </p>

    <!-- IMAGE: LDA PIPELINE -->
    <div class="row py-5">
      <div class="col-md-12">
        <div class="card p-0" style="border: none">
          <img src="img/NLP/pipeline.png" class="card-img-top" alt="...">
          <div class="card-body">
              <p style="font-weight: lighter; text-align: center;"><strong>FIGURE ## </strong>LDA Pipeline</p>
          </div>
        </div>
      </div>
    </div>
    <!-- IMAGE END -->

    <p>
      The <strong>DocumentAssembler</strong> transforms the input data to annotation format so that Spark can use it as input data - <mark>DOCUMENT</mark>. The <strong>Tokenizer</strong> splits lines in document into words - <mark>TOKEN</mark>. Finally, the Finisher transforms the annotation format of Spark NLP to a "human-readable" format. Normalizer (lowercasing), Lemmatizer, and StopWordsCleaners are skipped in this pipeline since they are already processed in the Data Cleaning step. The <strong>CountVectorizer</strong> is used to count the frequency of the each term in the document and provide input data for the LDA model. (<mark>featuresCol</mark> vector)


    </p>
    <p style="font-weight: lighter; text-align: left;">Reference: <a href="https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e">Topic Modelling with PySpark and Spark NLP</a> - Maria obedkova</p>
    <br>
    <p>
      The table below summarizes the result 15 topics and top 10 words per each topic.
    </p>




    <!--TOPIC MODELING SUMMARIZATION TABLE-->
    <table class="table table-bordered table-hover table-condensed">
      <thead style="text-align: center">
        <tr>
          <th title="Field #1" style="width: 10%">Topic #</th>
          <th title="Field #2" style="width: 90%">Topic Words</th>
        </tr>
      </thead>
      <tbody style="text-align: center">
        <tr>
          <td>1</td>
          <td>[lol, putin, russian, russia, dead, wait, years, good, guy, ukrainian]</td>
        </tr>
        <tr>
          <td>2</td>
          <td>[russian, ukraine, russians, russia, ukrainian, day, war, shit, make, ukrainians]</td>
        </tr>
        <tr>
          <td>3</td>
          <td>[russian, fuck, good, ukrainian, people, russia, guy, video, thing, lol]</td>
        </tr>
        <tr>
          <td>4</td>
          <td>[love, people, damn, ukraine, russia, world, war, god, good, lot]</td>
        </tr>
        <tr>
          <td>5</td>
          <td>[russian, ukraine, russia, tanks, air, tank, artillery, military, weapons, hit]</td>
        </tr>
        <tr>
          <td>6</td>
          <td>[post, ukraine, hope, video, people, reddit, russian, back, give, read]</td>
        </tr>
        <tr>
          <td>7</td>
          <td>[information, ukraine, message, rules, mod, reddit, issue, mail, vital, muted]</td>
        </tr>
        <tr>
          <td>8</td>
          <td>[ukraine, russian, bad, people, russia, youtu, troll, putin, army, time]</td>
        </tr>
        <tr>
          <td>9</td>
          <td>[russia, ukraine, war, people, russian, putin, country, nato, world, time]</td>
        </tr>
        <tr>
          <td>10</td>
          <td>[putin, shit, man, ukraine, russia, trump, russian, people, utm, war]</td>
        </tr>
      </tbody>
    </table>
    <p style="font-weight: lighter; text-align: center;"><strong>TABLE ## </strong>Topic words processed for 15 Topics (=k, clusters)</p>
    <!--TOPIC MODELING SUMMARIZATION TABLE-->


    <br>
    <br>
    <p>
      The plot below visualizes the result 15 topics and top 10 words per each topic.
    </p>

    <!-- IMAGE: LDA PIPELINE -->
    <div class="row py-2">
      <div class="col-md-12">
        <div class="card p-0" style="border: none">
          <img src="img/NLP/(temp)_topic_modeling_plot.png" class="card-img-top" alt="...">
          <div class="card-body">
              <p style="font-weight: lighter; text-align: center;"><strong>FIGURE ## </strong>Temporary figure</p>
          </div>
        </div>
      </div>
    </div>
    <!-- IMAGE END -->









    <br>
    <hr>
    <h4>Q2. (NLP) What are different languages other than English that can be analyzed in the Dataset? (language
      detection) Xinlu</h4>
    <p>
      Although it is easy to assume that most of the Reddit content is in English, based on the chosen topics, we target
      English, Ukrainian, and Russian as the languages of interest. To categorize the submission records by language, we
      use Spark NLP pre-trained language detection model <b>detect_language_375</b>. We obtain the prediction confidence
      toward these three languages for each submission and comment content. With the confidence values, we can define
      dummy variables for three languages by any desired bar. For the NLP part of our project, we subset our data by
      their language with 80% confidence. We conduct our common word analysis using PySpark SQL functions, following the
      process of tokenizing, aggregating by vocabulary, counting, and sorting. We obtain the TFIDF scores for our
      vocabularies using PySpark machine learning feature algorithms. The extraction and transformation tools include
      <b>Tokenizer</b>, <b>HashingTF</b>, <b>IDF</b>. We map the resulting TFIDF score to words by user-defined
      functions under the PySpark SQL environment. Since we are dealing with foreign languages, we also utilize Spark
      NLP pre-trained pipeline <b>translate_ru_en</b> and <b>translate_uk_en</b>, enabling translation from Russian and
      Ukrainian to English, respectively. When we only need the translation for word tokens, we use the pipelines start
      with document assembler, followed by the Spark Neural Machine Translation framework <b>Marian</b> models
      <b>opus_mt_ru_en</b> and <b>opus_mt_uk_en</b> respectively. The results we obtain give us insights into how the
      focus of the discussion in different languages varies from each other. However, since there is a significant gap
      between the volume of records in English and the volume of records in Russian and Ukrainian, we will not include
      records in these languages in our further explorations.
    </p>
      
      
      




    <br>
    <br>

    <br>
    <hr>
    <h4>Q3. Which topics/kinds of submissions/comments are more popular?</h4>

  <p>
    Applying the TFIDF method to our text data gives us more insight into online discussions. With the <b>pyspark</b> machine learning features and annotators, we feed our cleaned text data through the tokenizer. And then conduct hashing term frequency transformation for the tokenized column. HashingTF enables dimensionality reduction to the dataset, which is very helpful when working on large-sized data in a distributive manner. We then compute the inversed document frequency for each TF vector corresponding to the text records. By extracting function properties and annotator returns, we map the TFIDF score back to the textual word tokens and sort them to find the most important words. Please refer to <a
    href="https://github.com/gu-anly502/fall-2022-project-eda-adb-project-group-23/TFIDF.ipynb">this notebook</a> for calculating the TF-IDF scores.
  </p>
  <p>
    TFIDF helps us discard the dominance of words like country names and wars. The critical words give us different perspectives of the discussion. We can see political or geometrical representations like <i>European, German, and Belarus</i>. Words like <i>brigade</i>, <i> combat</i>, and </i> front</i> imply more detailed descriptions and discussion going on in the military-related discussion. The nickname "Dr. Eisenfaust." for the Mayer of Kyiv brings up a new political figure other than Putin. Instead of nouns, more adjectives appearing as important words bring out the discussion nature of Reddit. 
  </p>

  <br>

    <div class="table-responsive">
      <table class="table">
        <tr>
          <th>Language</th>
          <th>Reddit Subset</th>
          <th>Top 10 common words by word counts</th>
          <th>Top 10 important words by TF-IDF</th>
        </tr>
        <tr>
          <th rowspan="2" style="vertical-align: middle;">English</th>
          <th style="vertical-align: middle;">submission</th>
          <td><i>ukraine</i>, <i>russian</i>, <i>russia</i>, <i>ukrainian</i>, <i>war</i>, <i>putin</i>, <i>people</i>,
            <i>forces</i>, <i>military</i>, <i>russians</i>
          </td>
          <td><i>european</i>, <i>german</i>, <i>full</i>, <i>man</i>, <i>belarus</i>, <i>combat</i>, <i>avoided</i>,
            <i>left</i>, <i>brigade</i>, <i>front</i>
          </td>
        </tr>
        <tr>
          <th style="vertical-align: middle;">comment</th>
          <td><i>russia</i>, <i>ukraine</i>, <i>russian</i>, <i>war</i>, <i>people</i>, <i>putin</i>, <i>russians</i>,
            <i>ukrainian</i>, <i>time</i>, <i>good</i>
          </td>
          <td><i>hypercompetence</i>, <i>yourse</i>, <i>irepepctive</i>, <i>feeeeeeeelings</i>, <i>favorably</i>,
            <i>eisenfaust</i>, <i>cowy</i>, <i>trackers</i>, <i>correctlynused</i>, <i>isayevich</i>
          </td>
        </tr>
        <tr>
          <th rowspan="2" style="vertical-align: middle;">Russian</th>
          <th style="vertical-align: middle;">submission</th>
          <td><i>России(Russian Federation)</i>, <i>войны(War)</i>, <i>Путин(Putin)</i>, <i>США(United States
              dollars)</i>, <i>время(time)</i>, <i>своих(your own)</i>, <i>Зеленский(Green)</i>,
            <i>территории(Territory)</i>, <i>мир(peace)</i>, <i>Бабченко:(Babchenko:)</i>,
          </td>
          <td><i>других(Other)</i>, <i>работает(working)</i>, <i>города(Cities)</i>, <i>день(day)</i>,
            <i>помощи(assistance)</i>, <i>вся(All of them.)</i>, <i>детей(Children)</i>, <i>российской(Russian)</i>,
            <i>куклу(Doll)</i>, <i>мира(peace)</i>
          </td>
        </tr>
        <tr>
          <th style="vertical-align: middle;">comment</th>
          <td><i>России(Russian Federation)</i>, <i>Украине(Ukraine)</i>, <i>люди(people)</i>, <i>людей(of human
              beings)</i>, <i>войны(War)</i>, <i>Путин(Putin)</i>, <i>человек(Total population)</i>,
            <i>страны(Country)</i>, <i>НАТО(NATO)</i>, <i>США(United States dollars)</i>,
          </td>
          <td><i>начали(Start)</i>, <i>своим(your own)</i>, <i>ситуации(situation)</i>, <i>россияне(Russians)</i>,
            <i>самом(I'm not sure.)</i>, <i>шмольцы(Shh.)</i>, <i>человека(Human Rights)</i>, <i>страна(Country)</i>,
            <i>ребята…(Guys...)</i>, <i>мир(peace)</i>
          </td>
        </tr>
        <tr>
          <th rowspan="2" style="vertical-align: middle;">Ukrainian</th>
          <th style="vertical-align: middle;">submission</th>
          <td><i>України(Ukraine)</i>, <i>Росії(Russia)</i>, <i>війни(wars)</i>, <i>РФ(RF)</i>, <i>США(United
              States)</i>, <i>людей(people)</i>, <i>Слава(Glory)</i>, <i>вторгнення(invasion)</i>,
            <i>окупантів(Occupationals)</i>, <i>дітей(kids)</i>
          </td>
          <td><i>російська(grow up)</i>, <i>останнім(Permanent)</i>, <i>дорослих(The older)</i>, <i>дорозі(Dorose)</i>,
            <i>думкою(Thoughtful)</i>, <i>олександра(olexandra)</i>, <i>дізнатися(to know)</i>, <i>дія(the United
              Nations)</i>, <i>експертів(Suppliers)</i>, <i>translate(transfer)</i>
          </td>
        </tr>
        <tr>
          <th style="vertical-align: middle;">comment</th>
          <td><i>України(Ukraine)</i>, <i>людей(people)</i>, <i>війни(wars)</i>, <i>взагалі(generally)</i>,
            <i>питання(question)</i>, <i>країни(countries)</i>, <i>Слава(Glory)</i>, <i>Росії(Russia)</i>, <i>США(United
              States)</i>, <i>життя(life)</i>
          </td>
          <td><i>повномасштабному(large scale)</i>, <i>наприклад(Apparatus)</i>, <i>гроші(for a little while)</i>,
            <i>замінив(Deputy)</i>, <i>українців(Theft)</i>, <i>росії(and grow up)</i>, <i>microsoft(Microsoft)</i>,
            <i>supermarkety(Supermarkety)</i>, <i>автократії(autocratic)</i>, <i>аккаунт(account)</i>
          </td>
        </tr>
      </table>
      <p style="font-weight: lighter; text-align: center;"><strong>TABLE ##</strong>  Common/Important words found from applying tf-idf</p>
    </div>

    <br>
    

    <p>
      As we tagged all the submissions and comments by their languages, we split the datasets into English, Russian, and
      Ukrainian since they are part of the focus of our textual analysis.
    </p>
    <p>
      Common words can help us find what has frequently been brought up by people when making comments and posts under
      our selected subreddits. Inside the spark environment, we explode the body of the comments and submissions into
      single words. By aggregating all the vocabularies by their counts, we are able to sort them to find the most
      common words over our time interval of interest. Please refer to <a
        href="https://github.com/gu-anly502/fall-2022-project-eda-adb-project-group-23/common_words_submission_comment_language.ipynb">this
        notebook</a> for the code extracting common words.
    </p>
    <p>
      For the comments and submissions in English, terms related to <i>war</i> appear naturally. The top few are the
      cognate words of countries, which come with the territory of the subreddits we selected. While we may also
      interpret part of the counts for <i>russian</i> and <i>ukrainian</i> as referring to the people. Combined with the
      fact that the word <i>people</i> is one of the most common words, we can find the humane and compassionate nature
      of the discussion. Moreover, among all the political leaders involved in the conflict, Putin gets most of the
      attention.
    </p>
    <p>
      Beyond the top ten words we listed, NATO and the US are the two other political entities that appear the most in
      the discussion. We also find curse words appear in high frequency, implying an extremely sentimental tone among
      the posts.
    </p>
    <p>
      The common words for posts in Russian and Ukrainian include words like US dollars and populations, which could
      suggest different angles of discussions like finance and geopolitics. Words like <i>Green</i> and <i>Babchenko</i>
      possibly represent political entries and figures that are not familiar to foreign people outside of the
      confliction area. While again, we can find words that describe <i>peace</i> and <i>human beings</i>.
    </p>
  

    




    <p>
      It is a common practice to use correlation as preliminary analysis to test simple relationships between variables.
      Figure xx plots the Pearson correlations between all the timeseries we collected during the 8-month period.
    </p>

    <hr>



    <h4>Q8. How does each topic correlate to the commodity price changes?</h4>

    <p>
      It is a common practice to use correlation as preliminary analysis to test simple relationships between variables.
      Figure xx plots the Pearson correlations between all the timeseries we collected during the 8-month period.
    </p>

    <div class="row py-5">

      <div class="col-md-12">
        <div class="card p-0" style="border: none">
          <img src="img/NLP/correlations.png" class="card-img-top" alt="...">
          <div class="card-body">
            <p style="font-weight: lighter; text-align: center;"><strong>FIGURE ## </strong>Correlations between topics,
              sentiments, and commodity prices.</p>
          </div>
        </div>
      </div>
    </div>

    <p>
      The highlighting areas concentrate on the top-left and bottom-right corners, indicating there is strong
      relationship within
      commodity returns and reddit textual features, but not between them. Since our goal is to use reddit to predict
      commodity returns, a weak correlation between predictor and target implies we will need better features and a
      better-than-linear model.
    </p>
    <br>

    <hr>



    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>












  <!-- DO NOT TRASH BELOW -->

  </div> <!-- container -->


  <footer class="blog-footer">
    <p>
      <a href="#">Back to top</a>
    </p>
  </footer>



</body>

</html>
